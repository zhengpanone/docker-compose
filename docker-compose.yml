services:
  portainer:
    image: portainer/portainer:latest
    container_name: portainer
    ports:
      - "9000:9000"
    volumes:
      - /opt/dockerstore/portainer/data:/data
      - /var/run/docker.sock:/var/run/docker.sock

  rancher:
    image: rancher/rancher:v2.10-head
    privileged: true
    container_name: rancher
    restart: unless-stopped
    ports:
      - "18080:80" # HTTP 访问端口
      - "18443:443" # HTTPS 访问端口
    volumes:
      - /opt/dockerstore/rancher:/var/lib/rancher # 数据持久化
    environment:
      - CATTLE_BOOTSTRAP_PASSWORD=admin123 # 初始密码，启动后可更改

  registry:
    image: registry:latestEE
    container_name: registry
    hostname: registry
    ports:
      - "5001:5001"
    volumes:
      - /opt/dockerstore/registry:/var/lib/registry # 数据持久化
    environment:
      REGISTRY_HTTP_ADDR: :5001 # 配置 Docker Registry 监听的端口
    restart: unless-stopped # 除非手动停止，否则总是重启

  registry-browser:
    image: klausmeyer/docker-registry-browser:latest
    container_name: registry-browser
    ports:
      - "5002:8080"
    environment:
      SECRET_KEY_BASE: "123456"
      # 替换为你自己的内网IP
      DOCKER_REGISTRY_URL: "http://registry:5001/v2"
    depends_on:
      - registry # 确保 registry 服务先启动
    restart: unless-stopped

  mysql:
    hostname: mysql
    container_name: "mysql"
    #image: mysql:5.7
    image: mysql:8.0.24
    privileged: true
    restart: always
    ports:
      - "3306:3306"
    environment:
      MYSQL_ROOT_PASSWORD: "root"
      # MYSQL_ALLOW_EMPTY_PASSWORD: 'no'
      TZ: Asia/Shanghai
    volumes:
      - /opt/dockerstore/mysql/data:/var/lib/mysql
      - /opt/dockerstore/mysql/logs:/var/log/mysql
      - /opt/dockerstore/mysql/conf.d/:/etc/mysql/conf.d/
      # - ./init:/docker-entrypoint-initdb.d/   # sql初始化目录
    command: --max_connections=1000
      --character-set-server=utf8mb4
      --collation-server=utf8mb4_general_ci
      --default-authentication-plugin=mysql_native_password
    deploy:
      resources:
        limits:
          memory: 512m
          cpus: "0.5"
    networks:
      - my-network

  postgres:
    image: postgres:latest
    container_name: postgres
    hostname: postgres
    restart: always
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    ports:
      - 15432:5432
    volumes:
      - /opt/dockerstore/postgres/data:/var/lib/postgresql/data
      - /etc/localtime:/etc/localtime:ro #将外边时间直接挂载到容器内部，权限只读
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 512m
          cpus: "0.5"
    networks:
      - my-network

  # oracle_11g:
  #   image: registry.cn-hangzhou.aliyuncs.com/helowin/oracle_11g:latest
  #   container_name: oracle_11g
  #   ports:
  #     - 1521:1521
  # volumes:
  #   - /opt/dockerstore/oracle_11g/data:/home/oracle/app/oracle/oradata
  # restart: always

  redis:
    container_name: redis
    image: redis
    hostname: redis
    privileged: true
    restart: always
    ports:
      - "6379:6379"
    volumes:
      - /opt/dockerstore/redis/redis.conf:/etc/redis/redis.conf
      - /opt/dockerstore/redis/data:/data
    command: redis-server /etc/redis/redis.conf
    deploy:
      resources:
        limits:
          memory: 512m
          cpus: "0.5"
    networks:
      - my-network
  # 容器内执行：mongo -u root -p root --authenticationDatabase admin
  mongodb:
    container_name: mongodb
    hostname: mogodb
    image: mongo:5.0.10-focal
    restart: always
    ports:
      - "27017:27017"
    command:
      - "--auth"
      - "--wiredTigerCacheSizeGB=2"
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: root
    volumes:
      - "/opt/dockerstore/mongo/data:/data/db"
      - "/opt/dockerstore/mongo/logs:/var/log/mongodb"
      - "/opt/dockerstore/mongo/config:/etc/mongo"
      # - "/usr/share/zoneinfo/Asia/Shanghai:/etc/localtime"
    healthcheck:
      test: ["CMD", "mongo", "--eval", "db.adminCommand('ping')"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "10m" # 每个日志文件最大 10MB
        max-file: "5" # 最多保留 5 个日志文件

  mongo-express:
    image: mongo-express
    container_name: mongo-express
    # restart: always
    ports:
      - "8081:8081"
    environment:
      ME_CONFIG_MONGODB_SERVER: mongodb
      ME_CONFIG_MONGODB_ENABLE_ADMIN: false
      ME_CONFIG_MONGODB_ADMINUSERNAME: root
      ME_CONFIG_MONGODB_ADMINPASSWORD: root
      ME_CONFIG_BASICAUTH_USERNAME: root
      ME_CONFIG_BASICAUTH_PASSWORD: root
      # ME_CONFIG_MONGODB_URL: mongodb://root:root@mongo:27017/
    depends_on:
      - mongodb

  nginx:
    container_name: nginx
    image: nginx
    ports:
      - "9998:80"
    environment:
      TZ: Asia/Shanghai
    volumes:
      - /opt/dockerstore/nginx/html:/etc/nginx/html
      - /opt/dockerstore/nginx/conf.d:/etc/nginx/conf.d
      - /opt/dockerstore/nginx/log:/var/log/nginx

  # spug:
  #   image: openspug/spug-service
  #   container_name: spug
  #   privileged: true
  #   restart: always
  #   volumes:
  #     - /data/spug/service:/data/spug
  #     - /data/spug/repos:/data/repos
  #   ports:
  #     # 如果80端口被占用可替换为其他端口，例如: - "8000:80"
  #     - "8000:80"
  #   environment:
  #     - MYSQL_DATABASE=spug
  #     - MYSQL_USER=spug
  #     - MYSQL_PASSWORD=spug.cc
  #     - MYSQL_HOST=db
  #     - MYSQL_PORT=3306
  #   depends_on:
  #     - mysql

  nexus:
    image: sonatype/nexus
    container_name: nexus3
    # restart: always
    privileged: true
    environment:
      - TZ=Asia/Shanghai
    ports:
      - 8082:8081
    volumes:
      - /opt/dockerstore/nexus/nexus-data:/nexus-data

  gitlab:
    container_name: "gitlab"
    hostname: "gitlab"
    image: gitlab/gitlab-ce:latest
    privileged: true
    # restart: always
    ports:
      - "9980:80"
      - "2224:22"
      - "2443:443"
    environment:
      GITLAB_OMNIBUS_CONFIG: |
        external_url 'http://127.0.0.1:80'
        gitlab_rails['gitlab_shell_ssh_port'] = 2224
      TZ: Asia/Shanghai
    volumes:
      - /opt/dockerstore/gitlab/config:/etc/gitlab:z
      - /opt/dockerstore/gitlab/logs:/var/log/gitlab:z
      - /opt/dockerstore/gitlab/data:/var/opt/gitlab:z

  # neo4j:
  #   image: neo4j:latest
  #   container_name: neo4j
  #   restart: always
  #   environment:
  #     - NEO4J_AUTH=neo4j/neo4j # 默认用户名密码
  #     - NEO4J_dbms_memory_heap_maxSize=4G
  #   ports:
  #     - "7474:7474"
  #     - "7687:7687"
  #   volumes:
  #     - /opt/dockerstore/neo4j/data:/data
  #     - /opt/dockerstore/neo4j/config:/var/lib/neo4j/conf
  #     - /opt/dockerstore/neo4j/import:/var/lib/neo4j/import
  #     - /opt/dockerstore/neo4j/plugins:/plugins
  #     - /opt/dockerstore/neo4j/logs:/var/lib/neo4j/logs

  minio:
    container_name: minio
    image: minio/minio:RELEASE.2024-11-07T00-52-20Z
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    ports:
      - "9001:9001"
      - "9000:9000"
    volumes:
      - /opt/dockerstore/minio/data:/data
      # - /opt/dockerstore/minio/config:/root/.minio  # 配置存储
    command: minio server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9001/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  jenkins:
    image: jenkins/jenkins:2.462.2-lts-jdk17
    container_name: jenkins
    # restart: always
    privileged: true
    environment:
      - TZ=Asia/Shanghai
      - JAVA_OPTS=-Xmx4096m -Xms2048m # 调整JVM内存设置
    ports:
      - 8085:8080
      - 50000:50000
    volumes:
      - /opt/dockerstore/jenkins/jenkins-data:/var/jenkins_home
      - /opt/dockerstore/jenkins/jenkins-docker-certs:/certs/client:ro
      - /opt/dockerstore/jenkins/jenkins-logs:/var/log/jenkins # 日志文件持久化
      - /var/run/docker.sock:/var/run/docker.sock # 允许Jenkins构建Docker镜像
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"

  nacos:
    hostname: nacos
    # image: nacos/nacos-server:v2.1.2
    image: nacos/nacos-server:v2.4.2
    container_name: nacos
    privileged: true
    restart: always
    environment:
      TZ: Asia/Shanghai
      MODE: standalone
      PREFER_HOST_MODE: hostname #如果支持主机名可以使用hostname,否则使用ip，默认也是ip
      SPRING_DATASOURCE_PLATFORM: mysql #数据源平台 仅支持mysql或不保存empty
      MYSQL_SERVICE_HOST: mysql
      MYSQL_SERVICE_DB_NAME: nacos
      MYSQL_SERVICE_PORT: 3306
      MYSQL_SERVICE_USER: root
      MYSQL_SERVICE_PASSWORD: root
      MYSQL_DATABASE_NUM: 1
      NACOS_APPLICATION_PORT: 8848
      MYSQL_SERVICE_DB_PARAM: characterEncoding=utf8&connectTimeout=10000&socketTimeout=3000&autoReconnect=true&useSSL=false&serverTimezone=UTC
      NACOS_AUTH_TOKEN: 15baf51c4de4f206e9dece30afe5208a4095d5f7f6b17b80d1b0990ea934ba7e56a6dbfc3e93b1b89ae0359f55ca22c41ed4508f1444864ee803d46d412fe63b
    volumes:
      - /opt/dockerstore/nacos/logs:/home/nacos/logs
      - /opt/dockerstore/nacos/plugins:/home/nacos/plugins
      - /opt/dockerstore/nacos/conf:/home/nacos/conf
    ports:
      - "8848:8848"
      - "9848:9848"
    networks:
      - my-network

  consul:
    hostname: consul
    image: consul:1.15.4
    container_name: consul
    restart: always
    ports:
      - "8300:8300" # 这是 Consul 服务器节点之间通信的端口。主要用于服务器节点间的 RPC (远程过程调用) 通信，例如进行领导选举、状态复制等。
      - "8301:8301" # 这个端口被用于 Consul 节点之间的 Serf LAN (局域网) 通信。Serf 是一个用于集群成员管理、故障检测和编排的工具，Consul 利用它进行健康检查和跟踪集群中的成员。8301 端口使用 TCP 和 UDP 协议。
      - "8302:8302" # 这个端口被用于 Consul 节点间的 Serf WAN (广域网) 通信。如果您有多个数据中心，它们将通过此端口相互通信确定其他数据中心的 Consul 节点的健康状况。同样，8302 端口同时支持 TCP 和 UDP 协议。
      - "8500:8500" # Consul HTTP 界面所用的端口
      - "8600:8600/udp" # Consul DNS 服务器所用的端口
    command: agent -dev -ui -client 0.0.0.0 # -dev 表示启动一个开发模式的 Consul 服务，-ui 开启用户界面，-client 0.0.0.0 允许来自宿主机的所有IP连接至Consul服务
    volumes:
      - /opt/dockerstore/consul/data:/consul/data # 将宿主机当前目录下的 consul_data 目录挂载到容器的 /consul/data

    deploy:
      resources:
        limits:
          memory: 512m
          cpus: "0.5"
    networks:
      - my-network

  jaeger:
    container_name: jaeger
    image: jaegertracing/all-in-one:latest
    privileged: true
    ports:
      - "5775:5775/udp"
      - "6831:6831/udp"
      - "6832:6832/udp"
      - "5778:5778"
      - "16686:16686"
      - "14268:14268"
      - "14250:14250"
      - "9411:9411"
    environment:
      COLLECTOR_ZIPKIN_HTTP_PORT: 9411
    networks:
      - my-network

  sentinel:
    image: bladex/sentinel-dashboard:1.8.4
    container_name: sentinel-dashboard
    ports:
      - "8858:8858"
    environment:
      JAVA_OPTS: "-Dserver.port=8858 -Dcsp.sentinel.dashboard.server=localhost:8858 -Dproject.name=sentinel-dashboard"

  # Kong Migrations，用于初始化数据库
  kong-migrations:
    image: kong:3.5
    container_name: kong-migrations
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: postgres
      KONG_PG_USER: postgres
      KONG_PG_PASSWORD: postgres
    command: ["kong", "migrations", "bootstrap"]
    restart: on-failure
    networks:
      - my-network

  # Kong API 网关
  kong:
    image: kong:3.5
    container_name: kong
    # restart: always
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: postgres
      KONG_PG_USER: postgres
      KONG_PG_PASSWORD: postgres
      KONG_PROXY_ACCESS_LOG: /dev/stdout
      KONG_ADMIN_ACCESS_LOG: /dev/stdout
      KONG_PROXY_ERROR_LOG: /dev/stderr
      KONG_ADMIN_ERROR_LOG: /dev/stderr
      KONG_ADMIN_LISTEN: 0.0.0.0:8001
      KONG_LOG_LEVEL: info
    volumes:
      - /opt/dockerstore/kong/init_kong.sh:/docker-entrypoint.d/init_kong.sh
      - /opt/dockerstore/kong/config/kong.conf:/etc/kong/kong.conf # 挂载本地的kong.conf配置文件
      - /opt/dockerstore/kong/custom_plugins:/usr/local/kong/custom_plugins # 挂载本地自定义插件目录
    command: >
      kong start -c /etc/kong/kong.conf
    ports:
      - "8000:8000" # Proxy端口
      - "8002:8002" # gui
      - "8443:8443" # Proxy HTTPS端口
      - "8001:8001" # 管理API端口
      - "8444:8444" # 管理API HTTPS端口
    depends_on:
      consul:
        condition: service_started
      postgres:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/status"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - my-network

  konga:
    image: pantsel/konga:latest
    container_name: konga
    environment:
      NODE_ENV: production
      # DB_ADAPTER: postgres
      # DB_HOST: postgres
      # DB_PORT: 5432
      # DB_USER: postgres
      # DB_PASSWORD: postgres
      # DB_DATABASE: konga
      # DB_URI: postgres://postgres:postgres@postgres:5432/kong
    ports:
      - "1337:1337/tcp"
    depends_on:
      - kong
    volumes:
      - /opt/dockerstore/konga/kongadata:/app/kongadata
    networks:
      - my-network

  rabbitmq:
    image: rabbitmq:3.10-management
    container_name: rabbitmq
    ports:
      - "15673:15672"
      - "5673:5672"
    # hostname: rabbitmq
    # restart: always
    privileged: true
    environment:
      - TZ=Asia/Shanghai
      - LANG=en_US.UTF-8
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=admin
    volumes:
      - /opt/dockerstore/rabbitmq/data:/var/lib/rabbitmq # 防止log日志报failed to open log file
      - /opt/dockerstore/rabbitmq/conf/rabbitmq.conf/etc/rabbitmq/rabbitmq.conf
      - /opt/dockerstore/rabbitmq/log:/var/log/rabbitmq

  # grafana访问地址：http://ip地址:3000 默认登录账号密码：admin/admin
  # prometheus访问地址: http://ip地址:9090
  # exporter访问地址: http://ip地址:9100/metrics

  # 运行
  # docker-compose -f docker-compose-prometheus.yml -p prometheus up -d
  # 查看grafana日志
  # docker logs -fn10 prometheus-grafana
  # 开源的系统监控和报警系统
  prometheus:
    image: prom/prometheus:v2.34.0 # 原镜像`prom/prometheus:v2.34.0`
    container_name: prometheus
    restart: unless-stopped
    volumes:
      - /opt/dockerstore/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    command: "--config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus"
    ports:
      - "9090:9090"
    depends_on:
      - node-exporter
    networks:
      prometheus:
        ipv4_address: 172.22.0.11 # 为 Prometheus 服务指定静态 IP

  # 采集服务器层面的运行指标
  node-exporter:
    image: prom/node-exporter:v1.3.1 # 原镜像`prom/node-exporter:v1.3.1`
    container_name: prometheus-node-exporter
    restart: unless-stopped
    ports:
      - "9100:9100"
    networks:
      prometheus:
        ipv4_address: 172.22.0.22 # 为 Node Exporter 服务指定静态 IP

  # 用于UI展示
  # https://grafana.com/docs/grafana/latest/installation/docker
  grafana:
    image: grafana:8.0.0 # 原镜像`grafana/grafana:8.0.0`
    container_name: prometheus-grafana
    restart: unless-stopped
    ports:
      - "3000:3000"
    volumes:
      - "/opt/dockerstore/grafana/grafana.ini:/etc/grafana/grafana.ini" # 邮箱配置
    #      - "./prometheus/grafana/grafana-storage:/var/lib/grafana"
    #      - "./prometheus/grafana/public:/usr/share/grafana/public" # 这里面可处理汉化包 可参考 https://github.com/WangHL0927/grafana-chinese
    #      - "./prometheus/grafana/conf:/usr/share/grafana/conf"
    #      - "./prometheus/grafana/log:/var/log/grafana"
    #      - "/etc/localtime:/etc/localtime"
    environment:
      GF_EXPLORE_ENABLED: "true"
      GF_SECURITY_ADMIN_PASSWORD: "admin"
      GF_INSTALL_PLUGINS: "grafana-clock-panel,grafana-simple-json-datasource,alexanderzobnin-zabbix-app"
      # 持久化到mysql数据库
      GF_DATABASE_URL: "mysql://root:root@mysql:3306/grafana" # TODO 修改
    depends_on:
      - prometheus
      - mysql
    networks:
      prometheus:
        ipv4_address: 172.22.0.33

  # 生成 elastic-certificates.p12
  cert_generator:
    image: elasticsearch:7.14.1
    container_name: cert_generator
    environment:
      - discovery.type=single-node
    # 生成秘钥
    # ./bin/elasticsearch-certutil ca

    #./bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12
    # volumes:
    #   - /opt/dockerstore/elasticsearch/config/:/usr/share/elasticsearch/config

  es01:
    image: elasticsearch:7.14.1
    container_name: es01
    environment:
      - node.name=es01
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=es02,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - /opt/dockerstore/elasticsearch/data/es01-data/:/usr/share/elasticsearch/data
      - /opt/dockerstore/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
      - /opt/dockerstore/elasticsearch/config/elastic-certificates.p12:/usr/share/elasticsearch/config/elastic-certificates.p12
    ports:
      - 9201:9200
  es02:
    image: elasticsearch:7.14.1
    container_name: es02
    environment:
      - node.name=es02
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=es01,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    volumes:
      - /opt/dockerstore/elasticsearch/data/es02-data/:/usr/share/elasticsearch/data
      - /opt/dockerstore/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
      - /opt/dockerstore/elasticsearch/config/elastic-certificates.p12:/usr/share/elasticsearch/config/elastic-certificates.p12
    ports:
      - 9202:9200
    ulimits:
      memlock:
        soft: -1
        hard: -1

  es03:
    image: elasticsearch:7.14.1
    container_name: es03
    environment:
      - node.name=es03
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=es01,es02
      - cluster.initial_master_nodes=es01,es02,es03
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - /opt/dockerstore/elasticsearch/data/es03-data/:/usr/share/elasticsearch/data
      - /opt/dockerstore/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
      - /opt/dockerstore/elasticsearch/config/elastic-certificates.p12:/usr/share/elasticsearch/config/elastic-certificates.p12
    ports:
      - 9203:9200

  kibana:
    image: kibana:7.14.1
    container_name: kibana01
    volumes:
      - /opt/dockerstore/kibana/config/kibana.yml:/usr/share/kibana/config/kibana.yml
    environment:
      - ELASTICSEARCH_HOSTS=http://es01:9200
      - ELASTICSEARCH_URL=http://es01:9200
      - I18N_LOCALE=zh-CN
    ports:
      - "5602:5601"
    depends_on:
      - es01

  # 监控集群信息
  cerebro:
    image: lmenezes/cerebro:0.9.4
    container_name: cerebro
    environment:
      - CEREBRO_HOSTS=http://es01:9200
    ports:
      - "9000:9000"

networks:
  my-network:
    driver: bridge
  prometheus:
    driver: bridge
    ipam:
      config:
        - subnet: 172.22.0.0/24
